<!doctype html>

<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Kate Sanders</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1 style="font-size:35px;margin-bottom:20px;"><a href="">Kate Sanders</a></h1>
        <img src="assets/kate_sanders.jpg" style="width:60%">
        <p class="view" style="margin-bottom:5px; margin-top:10px">ksande25@jhu.edu</p>
        <p class="view" style="margin-bottom:5px"><a href="https://scholar.google.com/citations?user=VJFrfM0AAAAJ">Google Scholar</a></p>
        <p class="view" style="margin-bottom:5px"><a href="https://www.linkedin.com/in/kate-sanders-395725146/">LinkedIn</a></p>
        <p class="view"><a href="assets/cv.pdf">CV</a></p>
      </header>
      <section>

        <p><small><i>Site last updated February 2026</i></small></p>
        
        <!--<div style="background-color:#e3f2fa;padding:0px;border:solid #c7e8fc;border-radius: 10px;"><p style="margin:3px;padding-left:7px;font-size:15px;">⭐️&nbsp; <a href="assets/cv.pdf" style="color:#069;"><b>I'm currently on the industry job market!</b> I am looking for a full-time research role in industry that involves reasoning systems and NLP. I am particularly interested in multimodality (image and video), AI for science, evaluation, factuality, and retrieval. If you think there might be a match, please feel free to reach out to me via email (ksande25@jhu.edu) or LinkedIn! </a>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;See below for an overview of my interests and experience.</p></div>
          <br>-->
          <!--<hr style="margin-bottom:15px;margin-top:20px;padding-top:3px;">-->

        <p>Hello! I am an incoming AI researcher at Microsoft interested in post-training, reasoning, and multimodality. I did my Ph.D. at <a href="https://www.clsp.jhu.edu/">Johns Hopkins University</a> and was advised by <a href="https://www.cs.jhu.edu/~vandurme/">Professor Benjamin Van Durme</a>. My thesis work centered on multimodal event semantics.</p>

        <p>I spent this past summer as an intern at AWS designing reward functions to train reasoning models, where I was mentored by <a href="http://nweir127.github.io">Nathaniel Weir</a>. The previous summer, I co-organized and facilitated the 10-week <a href="https://hltcoe.jhu.edu/research/scale/scale-2024/">SCALE 2024 Summer Research Workshop</a> at the <a href="https://hltcoe.jhu.edu/">HLTCOE</a>. Before starting my Ph.D., I received my BA in Computer Science from UC Berkeley where I did robotics research at the <a href="http://autolab.berkeley.edu/">UC Berkeley AUTOLab</a>, advised by <a href="https://goldberg.berkeley.edu/">Professor Ken Goldberg</a>.</p>

        <!--<hr style="margin-bottom:-10px;padding-top:3px;">-->

        <h2>Research</h2>
        <!--<p><i><a href="https://scholar.google.com/citations?user=VJFrfM0AAAAJ"><b>Papers</b></a> | <a href=""><b>Posters</b></a> | <a href=""><b>Slides</b></a></i></p>-->

        <p>Please visit my <a href="https://scholar.google.com/citations?user=VJFrfM0AAAAJ">Google Scholar profile</a> for a full list of publications.</p>

        <p>These are a few main areas I published in during my Ph.D., alongside some representative papers:</p>
        
        <details>
          <summary>
            <span class="icon">▷</span>
            Benchmarks for complex visual event understanding
          </summary>

          <div class="pdiv" style="margin-left:20px;margin-right:20px;margin-top:20px;">
            <p>I have spent the last few years working with collaborators to develop ways of thinking about and formulating events in visual data. This has inspired the development of a collection of benchmarks designed to evaluate an agent's ability to recognize and explain these events across different languages and regions. This effort began as a small video retrieval task <b>[1]</b> that was then extended to a full event extraction benchmark <b>[2]</b>. We emphasize the notion of "partially-defined events" in this work: In visual content, we argue that it is critical to model the epistemic and aleatoric uncertainty associated with identifying events that are more commonly described with natural language <b>[3]</b>. More recent extensions to this line of work include a much larger video retrieval dataset built on these initial benchmarks <b>[4]</b> and a report generation benchmark <b>[5]</b> that highlights the difficulty of piecing together multiple videos that only <i>together</i> describe some partially-defined event.</p>
          </div>
  
            <hr style="margin-bottom:-10px">

          <p>
            <ol>
    <li><b>Sanders, K.*</b>, Etter, D.*, Kriz, R.*, Van Durme, B. <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/a054ff49751dbc991ec30ae479397c3d-Abstract-Datasets_and_Benchmarks.html">MultiVENT: Multilingual Videos of Events with Aligned Natural Text</a>. NeurIPS 2023 D&B.</li>

    <li><b>Sanders, K.*</b>, Kriz, R.*, Etter, D.*, Recknor, H., Martin, A., Carpenter, C., Lin, J., Van Durme, B. <a href="https://arxiv.org/abs/2410.05267">Grounding Partially-Defined Events in Multimodal Data</a>. EMNLP 2024 Findings.</li>

    <li><b>Sanders, K.</b>, Van Durme, B. (2024). <a href="https://openaccess.thecvf.com/content/CVPR2024W/VDU/html/Sanders_A_Survey_of_Video_Datasets_for_Grounded_Event_Understanding_CVPRW_2024_paper.html">A Survey of Video Datasets for Grounded Event Understanding</a>. CVPR 2024 Workshops.</li>

    <li>Kriz, R.*, <b>Sanders, K.*</b>, Etter, D., Murray, M., Carpenter, C., Recknor, H., Blasco, J., Martin, A., Yang, E., Van Durme, B. <a href="https://arxiv.org/abs/2410.11619v1">"MultiVENT 2.0: A Massive Multilingual Benchmark for Event-Centric Video Retrieval</a>. CVPR 2025.</li>

    <li>Martin, A., Kriz, R., Walden, W., <b>Sanders, K.</b>, Recknor H., Yang, E., Ferraro, F., Van Durme, B.
    <a href="https://arxiv.org/pdf/2504.00939">WikiVideo: Article Generation from Multiple Videos</a>. 2025 arXiv preprint.</li>

    </ol>
        </p>
        </details>

        <details>
          <summary>
            <span class="icon">▷</span>
            Interpretable and factually accurate reasoning
          </summary>

          <div class="pdiv" style="margin-left:20px;margin-right:20px;margin-top:20px;">
            <p>I am very lucky to be able to work on factuality and transparency research with so many of my labmates. We have extended ideas in informal logic to assess the quality of compositional entailment in neuro-symbolic reasoning systems <b>[1]</b>, and extended these reasoning systems to transparently verify content in the video-language domain <b>[2]</b>. I built on this multimodal framework to incorporate uncertainty modeling and to generalize to modalities beyond vision and text <b>[3]</b>. I also worked with labmates to develop a subclaim selection comoponent to improve the trustworthiness of factuality metrics like FActScore <b>[4]</b>, and evaluate LLM's abilities to verify claims in scientific reports <b>[5]</b>.</p>
          
          <p>More publications on trustworthy reasoning are coming soon!</p>
          </div>
  
            <hr style="margin-bottom:-10px">

          <p><ol>
            <li>Weir, N., <b>Sanders, K.</b>, Weller, O., Sharma, S., Jiang, D., Jiang, Z., ..., Van Durme, B. <a href="https://arxiv.org/abs/2402.14798">Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic</a>. EMNLP 2024.</li>

            <li><b>Sanders, K.</b>, Weir, N., Van Durme, B. <a href="https://arxiv.org/abs/2402.19467">TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning</a>. EMNLP 2024.</li>

            <li><b>Sanders, K.</b>, Van Durme, B.
              <a href="https://arxiv.org/pdf/2504.03640?">Bonsai: Interpretable Tree-Adaptive Grounded Reasoning</a>. 2025 arXiv preprint.</li>

            <li>Jiang, Z., Zhang, J., Weir, N., Ebner, S., Wanner, M., <b>Sanders, K.</b>, Khashabi, D., Liu, A., Van Durme, B. (2024). <a href="https://arxiv.org/abs/2407.03572">Core: Robust Factual Precision Scoring with Informative Sub-Claim Identification</a>. ACL 2025 Findings.</li>

            <li>Ou, J.*, Walden, W.*, <b>Sanders, K.</b>, Jiang, Z., Sun, K., Cheng, J., ..., Van Durme, B.
              <a href="https://arxiv.org/pdf/2503.21717">CLAIMCHECK: How Grounded are LLM Critiques of Scientific Papers?</a> EMNLP 2025 Findings.</li>
      
  </ol>
  </p>
        </details>

        <details>
          <summary>
            <span class="icon">▷</span>
            Video retrieval for real-world queries
          </summary>

          <div class="pdiv" style="margin-left:20px;margin-right:20px;margin-top:20px;"> 
            <p>I spent a summer at SCALE 2024 hacking on the MultiVENT video retrieval dataset to find new ways to perform video retrieval on more realistic, real-world queries pertaining to visual events. In addition to the MultiVENT 2.0 benchmark (which served as a shared task for our ACL 2025 workshop on multimodal retrieval and generation) we came up with some methods papers for tackling these sorts of problems: Video-ColBERT <b>[1]</b> draws inspiration from late interaction retrieval methods to combine token-wise interaction on static frame features and temporally contextualized video features for improved retrieval. MMMORRF <b>[2]</b> takes a separate approach, balancing the contributions of both vision and audio by establishing distinct data processing pipelines for each and fusing them through modality-aware weighted reciprocal rank fusion. FORTIFY <b>[3]</b> addresses the challenging problem of modeling OCR content for retrieval by leveraging generative models to rewrite and synthesize these fragments as noisy, multilingual documents.</p></div>
  
            <hr style="margin-bottom:-10px">
          
          <p>
            <ol>
            <li>Reddy, A., Martin, A., Yang, E., Yates, A., <b>Sanders, K.</b>, Murray, K., Kriz, R., M de Melo, C., Van Durme, B., Chellappa, R.
    <a href="https://arxiv.org/pdf/2503.19009">Video-ColBERT: Contextualized Late Interaction for Text-to-Video Retrieval</a>. CVPR 2025.</li>

    <li>Samuel, S., DeGenaro, D., Guallar-Blasco, J., <b>Sanders, K.</b>, Eisape, O., ..., Kriz, R.
    <a href="https://arxiv.org/pdf/2503.20698">MMMORRF:
Multimodal Multilingual MOdularized Reciprocal Rank Fusion</a>. SIGIR 2025 Demo.</li>

<li>DeGenaro, D., Yang, E., Etter, D., Carpenter, C., <b>Sanders, K.</b>, Martin, A., Murray, K., Kriz, R. <a href="https://aclanthology.org/2025.magmar-1.13.pdf">
  FORTIFY: Generative Model Fine-tuning with ORPO for ReTrieval Expansion of InFormal NoisY Text</a> ACL 2025 Workshops.</li>
        </ol></p>
        </details>

        <details>
          <summary>
            <span class="icon">▷</span>
            Miscellaneous
          </summary>
          
          <div class="pdiv" style="margin-left:20px;margin-right:20px;margin-top:20px;">
          <p>I recently worked on a fun collaboration with <a href="https://groups.csail.mit.edu/cap/">MIT CSAIL's Computer-Aided Programming Group</a> where we explored how well LLMs and reasoning models can learn low-level language syntax via in-context learning <b>[1]</b>. I also have experience developing benchmarks for web agents <b>[2]</b> and for assessing the calibration of vision classification models via human judgment solicitation <b>[3]</b>. I published a handful of robotics papers while in undergrad: My two favorites concern identifying failure modes for bin-picking systems and suction grippers <b>[4, 5]</b>.</p></div>

          <hr style="margin-bottom:-10px">

          <p>
            <ol>
              <li>Gupta, K., <b>Sanders, K.</b>, Solar-Lezama, A. <a href="https://arxiv.org/pdf/2501.02825">Randomly Sampled Language Reasoning Problems Reveal Limits of LLMs</a>. ICLR 2025 Workshops. </li>

              <li>Xu, K., Kordi, Y., Nayak, T., Asija, A., Wang, Y., <b>Sanders, K.</b>, Byerly, A., Zhang, J., Van Durme, B., Khashabi, D. <a href="https://arxiv.org/abs/2403.11905">Tur[k]ingBench: A Challenge Benchmark for Web Agents</a>. NAACL 2025.</li>

              <li><b>Sanders, K.</b>, Kriz, R., Liu, A., Van Durme, B. <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/11e3e0f1b29dcd31bd0952bfc1357f68-Abstract-Datasets_and_Benchmarks.html">Ambiguous Images With Human Judgments for Robust Visual Event Classification</a>. NeurIPS 2022 D&B.</li>

              <li><b>Sanders, K.</b>, Danielczuk, M., Mahler, J., Tanwani, A., Goldberg, K. <a href="https://ieeexplore.ieee.org/abstract/document/9216844">Non-Markov Policies to Reduce Sequential Failures in Robot Bin Picking</a>. CASE 2020.</li>

              <li>Huh, T. M., <b>Sanders, K.</b>, Danielczuk, M., Li, M., Chen, Y., Goldberg, K., Stuart, H. S. <a href="https://ieeexplore.ieee.org/abstract/document/9635852">A Multi-Chamber Smart Suction Cup for Adaptive Gripping and Haptic Exploration</a>. IROS 2021.</li>
            </p>
        </ol></p>
        </details>



        <h2>News</h2>
        <table style="width:100%">
          <tr>
            <td style="width:12%"><b>Feb. 2026</b></td>
            <td>Defended my dissertation on event semantics for video!</td>
          </tr>
          <tr>
            <td style="width:12%"><b>Jan. 2026</b></td>
            <td><a href="https://arxiv.org/pdf/2504.03640?">Bonsai</a> was presented at AAAI 2026.</td>
          </tr>
          <tr>
            <td style="width:12%"><b>Dec. 2025</b></td>
            <td>Happy to announce that I will be joining Microsoft's Copilot Tuning team in 2026.</td>
          </tr>
          <tr>
            <td style="width:12%"><b>Nov. 2025</b></td>
            <td><a href="https://nlp.jhu.edu/magmar/">CLAIMCHECK</a> was presented at EMNLP 2025.</td>
          </tr>
        </table>

      </section>
      <footer>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>

  </body>
</html>
