<!doctype html>

<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Kate Sanders</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="">Kate Sanders</a></h1>
        <img src="assets/kate_sanders.jpg" style="width:80%">
        <p class="view" style="margin-bottom:5px; margin-top:10px">ksande25@jhu.edu</p>
        <p class="view" style="margin-bottom:5px"><a href="https://scholar.google.com/citations?user=VJFrfM0AAAAJ">Google Scholar</a></p>
        <p class="view" style="margin-bottom:5px"><a href="https://www.linkedin.com/in/kate-sanders-395725146/">LinkedIn</a></p>
        <p class="view"><a href="assets/cv.pdf">CV</a></p>
      </header>
      <section>

        <p><small><i>Site last updated August 2025</i></small></p>

        <p>I am a final year Ph.D. student at the <a href="https://www.clsp.jhu.edu/">JHU Center for Language and Speech Processing</a> at Johns Hopkins University. I am advised by <a href="https://www.cs.jhu.edu/~vandurme/">Professor Benjamin Van Durme</a>. During my Ph.D., I have been researching transparent and reliable reasoning, multimodal understanding, and uncertainty.</p>

        <p>I spent this past summer as an intern at AWS designing reward functions to train reasoning models, where I was lucky to be mentored by <a href="http://nweir127.github.io">Nathaniel Weir</a>. The previous summer, I co-organized and facilitated the 10-week <a href="https://hltcoe.jhu.edu/research/scale/scale-2024/">SCALE 2024 Summer Research Workshop</a> at the <a href="https://hltcoe.jhu.edu/">HLTCOE</a>.</p>
          
        <p>Before starting my Ph.D., I received my BA in Computer Science from UC Berkeley where I conducted AI and robotics research at the <a href="http://autolab.berkeley.edu/">UC Berkeley AUTOLab</a> and was advised by <a href="https://goldberg.berkeley.edu/">Professor Ken Goldberg</a>.</p>



        <h2>Research areas</h2>
        <p>Please visit my <a href="https://scholar.google.com/citations?user=VJFrfM0AAAAJ">Google Scholar profile</a> for a full list of publications.</p>

        <details>
          <summary>
            <span class="icon">▷</span>
            Benchmarks for complex visual event understanding
          </summary>

          <div class="pdiv" style="margin-left:20px;margin-right:20px;margin-top:20px;">
            <p>I have spent the last few years working with collaborators to develop ways of thinking about and formulating events in visual data. This has inspired the development of a collection of benchmarks designed to evaluate agents' abilities to recognize and explain these events across different languages and cultures. This effort began as a small video retrieval task that was then extended to a full event extraction benchmark. We emphasize the notion of "partially-defined events" in this paper: In visual content, we argue that it is critical to model the epistemic and aleatoric uncertainty associated with identifying events more commonly described through natural language. More recent extensions to this line of work include a massive video retrieval dataset built on these initial benchmarks that better mirrors datasets developed by the information retrieval community and a report generation benchmark that highlights the difficulty of piecing together multiple videos that only <i>together</i> help to describe some partially-defined event.</p>
          </div>
  
            <hr style="margin-bottom:-10px">

          <p>
            <ol>
    <li><b>Sanders, K.*</b>, Etter, D.*, Kriz, R.*, Van Durme, B. <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/a054ff49751dbc991ec30ae479397c3d-Abstract-Datasets_and_Benchmarks.html">MultiVENT: Multilingual Videos of Events with Aligned Natural Text</a>. NeurIPS 2023 D&B.</li>

    <li><b>Sanders, K.*</b>, Kriz, R.*, Etter, D.*, Recknor, H., Martin, A., Carpenter, C., Lin, J., Van Durme, B. <a href="https://arxiv.org/abs/2410.05267">Grounding Partially-Defined Events in Multimodal Data</a>. EMNLP 2024 Findings.</li>

    <li>Kriz, R.*, <b>Sanders, K.*</b>, Etter, D., Murray, M., Carpenter, C., Recknor, H., Blasco, J., Martin, A., Yang, E., Van Durme, B. <a href="https://arxiv.org/abs/2410.11619v1">"MultiVENT 2.0: A Massive Multilingual Benchmark for Event-Centric Video Retrieval</a>. CVPR 2025.</li>

    <li>Martin, A., Kriz, R., Walden, W., <b>Sanders, K.</b>, Recknor H., Yang, E., Ferraro, F., Van Durme, B.
    <a href="https://arxiv.org/pdf/2504.00939">WikiVideo: Article Generation from Multiple Videos</a>. 2025 arXiv preprint.</li>

    <li><b>Sanders, K.</b>, Van Durme, B. (2024). <a href="https://openaccess.thecvf.com/content/CVPR2024W/VDU/html/Sanders_A_Survey_of_Video_Datasets_for_Grounded_Event_Understanding_CVPRW_2024_paper.html">A Survey of Video Datasets for Grounded Event Understanding</a>. CVPR 2024 Workshops.</li>
    </ol>
        </p>
        </details>

        <details>
          <summary>
            <span class="icon">▷</span>
            Interpretable and factually accurate reasoning
          </summary>

          <div class="pdiv" style="margin-left:20px;margin-right:20px;margin-top:20px;">
            <p>I am very lucky to have been able to spend a lot of time discussing notions of factuality and transparency with my labmates. I collaborated to extend ideas in informal logic to assess the quality of compositional entailment in neuro-symbolic reasoning systems, and extended these reasoning systems to transparently verify content in the video-language domain. I built on this framework to incorporate uncertainty modeling (described further in "benchmarks for complex visual event understanding" above) and generalize to modalities beyond vision and text. I also worked with labmates to develop a subclaim selection comoponent to improve the trustworthiness of factuality metrics like FActScore, and evaluate LLM's abilities to verify claims in scientific reports.</p>
          
          <p>More publications on trustworthy reasoning are coming soon!</p>
          </div>
  
            <hr style="margin-bottom:-10px">

          <p><ol>
            <li>Weir, N., <b>Sanders, K.</b>, Weller, O., Sharma, S., Jiang, D., Jiang, Z., ..., Van Durme, B. <a href="https://arxiv.org/abs/2402.14798">Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic</a>. EMNLP 2024.</li>

            <li><b>Sanders, K.</b>, Weir, N., Van Durme, B. <a href="https://arxiv.org/abs/2402.19467">TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning</a>. EMNLP 2024.</li>

            <li><b>Sanders, K.</b>, Van Durme, B.
              <a href="https://arxiv.org/pdf/2504.03640?">Bonsai: Interpretable Tree-Adaptive Grounded Reasoning</a>. 2025 arXiv preprint.</li>

            <li>Jiang, Z., Zhang, J., Weir, N., Ebner, S., Wanner, M., <b>Sanders, K.</b>, Khashabi, D., Liu, A., Van Durme, B. (2024). <a href="https://arxiv.org/abs/2407.03572">Core: Robust Factual Precision Scoring with Informative Sub-Claim Identification</a>. ACL 2025 Findings.</li>

            <li>Ou, J.*, Walden, W.*, <b>Sanders, K.</b>, Jiang, Z., Sun, K., Cheng, J., ..., Van Durme, B.
              <a href="https://arxiv.org/pdf/2503.21717">CLAIMCHECK: How Grounded are LLM Critiques of Scientific Papers?</a> 2025 arXiv preprint.</li>
      
  </ol>
  </p>
        </details>

        <details>
          <summary>
            <span class="icon">▷</span>
            Video retrieval for real-world queries
          </summary>

          <div class="pdiv" style="margin-left:20px;margin-right:20px;margin-top:20px;">
            <p>I spent a summer at SCALE 2024 hacking on the MultiVENT video retrieval dataset to find new ways to perform video retrieval on more realistic, real-world queries pertaining to visual events. In addition to the MultiVENT 2.0 benchmark (which served as a shared task for our ACL 2025 workshop on multimodal retrieval and generation) we came up with some methods papers for tackling these sorts of problems: Video-ColBERT draws inspiration from late interaction retrieval methods to combine token-wise interaction on static frame features and temporally contextualized video features for improved retrieval. MMMORRF takes a separate approach, balancing the contributions of both vision and audio by establishing distinct data processing pipelines for each and fusing them through modality-aware weighted reciprocal rank fusion. FORTIFY addresses the challenging problem of modeling OCR content for retrieval by leveraging generative models to rewrite and synthesize these fragments as noisy, multilingual documents.</p></div>
  
            <hr style="margin-bottom:-10px">
          
          <p>
            <ol>
            <li>Reddy, A., Martin, A., Yang, E., Yates, A., <b>Sanders, K.</b>, Murray, K., Kriz, R., M de Melo, C., Van Durme, B., Chellappa, R.
    <a href="https://arxiv.org/pdf/2503.19009">Video-ColBERT: Contextualized Late Interaction for Text-to-Video Retrieval</a>. CVPR 2025.</li>

    <li>Samuel, S., DeGenaro, D., Guallar-Blasco, J., <b>Sanders, K.</b>, Eisape, O., ..., Kriz, R.
    <a href="https://arxiv.org/pdf/2503.20698">MMMORRF:
Multimodal Multilingual MOdularized Reciprocal Rank Fusion</a>. SIGIR 2025 Demo.</li>

<li>DeGenaro, D., Yang, E., Etter, D., Carpenter, C., <b>Sanders, K.</b>, Martin, A., Murray, K., Kriz, R. <a href="https://aclanthology.org/2025.magmar-1.13.pdf">
  FORTIFY: Generative Model Fine-tuning with ORPO for ReTrieval Expansion of InFormal NoisY Text</a> ACL 2025 Workshops.</li>
        </ol></p>
        </details>

        <details>
          <summary>
            <span class="icon">▷</span>
            Miscellaneous
          </summary>
          
          <div class="pdiv" style="margin-left:20px;margin-right:20px;margin-top:20px;">
          <p>I recently worked on a fun collaboration with <a href="https://groups.csail.mit.edu/cap/">MIT CSAIL's Computer-Aided Programming Group</a> where we explored how well LLMs and reasoning models can learn low-level language syntax via in-context learning <b>[1]</b>. I also have experience developing benchmarks for web agents <b>[2]</b> and for assessing the calibration of vision classification models via human judgment solicitation <b>[3]</b>. I published a handful of robotics papers while in undergrad: My two favorites concern identifying failure modes for bin-picking systems and suction grippers <b>[4, 5]</b>.</p></div>

          <hr style="margin-bottom:-10px">

          <p>
            <ol>
              <li>Gupta, K., <b>Sanders, K.</b>, Solar-Lezama, A. <a href="https://arxiv.org/pdf/2501.02825">Randomly Sampled Language Reasoning Problems Reveal Limits of LLMs</a>. ICLR 2025 Workshops. </li>

              <li>Xu, K., Kordi, Y., Nayak, T., Asija, A., Wang, Y., <b>Sanders, K.</b>, Byerly, A., Zhang, J., Van Durme, B., Khashabi, D. <a href="https://arxiv.org/abs/2403.11905">Tur[k]ingBench: A Challenge Benchmark for Web Agents</a>. NAACL 2025.</li>

              <li><b>Sanders, K.</b>, Kriz, R., Liu, A., Van Durme, B. <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/11e3e0f1b29dcd31bd0952bfc1357f68-Abstract-Datasets_and_Benchmarks.html">Ambiguous Images With Human Judgments for Robust Visual Event Classification</a>. NeurIPS 2022 D&B.</li>

              <li><b>Sanders, K.</b>, Danielczuk, M., Mahler, J., Tanwani, A., Goldberg, K. <a href="https://ieeexplore.ieee.org/abstract/document/9216844">Non-Markov Policies to Reduce Sequential Failures in Robot Bin Picking</a>. CASE 2020.</li>

              <li>Huh, T. M., <b>Sanders, K.</b>, Danielczuk, M., Li, M., Chen, Y., Goldberg, K., Stuart, H. S. <a href="https://ieeexplore.ieee.org/abstract/document/9635852">A Multi-Chamber Smart Suction Cup for Adaptive Gripping and Haptic Exploration</a>. IROS 2021.</li>
            </p>
        </ol></p>
        </details>

        <h2>News</h2>
        <table style="width:100%">
          <tr>
            <td style="width:12%"><b>Aug 2025</b></td>
            <td>Co-organized the first workshop on multimodal retrieval and generation at ACL 2025.</td>
          </tr>
          <tr>
            <td style="width:12%"><b>July 2025</b></td>
            <td><a href="https://arxiv.org/abs/2407.03572">CORE</a> and <a href="https://aclanthology.org/2025.magmar-1.13.pdf">
              FORTIFY</a> presented @ ACL 2025.</td>
          </tr>
          <tr>
            <td style="width:12%"><b>July 2025</b></td>
            <td><a href="https://arxiv.org/pdf/2503.20698">MMMORRF</a> presented @ SIGIR 2025.</td>
          </tr>
          <tr>
            <td style="width:12%"><b>June 2025</b></td>
            <td><a href="https://arxiv.org/abs/2410.11619v1">MultiVENT 2.0</a> and <a href="https://arxiv.org/pdf/2503.19009">Video-ColBERT</a> presented @ CVPR 2025.</td>
          </tr>
          <tr>
            <td style="width:12%"><b>May 2025</b></td>
            <td>Interning at AWS to work on reasoning model reward functions.</td>
          </tr>
          <tr>
            <td style="width:12%"><b>April 2025</b></td>
            <td><a href="https://arxiv.org/pdf/2501.02825">Randomly Sampled Language Reasoning Problems</a> presented @ ICLR 2025 VerifAI workshop.</td>
          </tr>
          <tr>
            <td style="width:12%"><b>April 2025</b></td>
            <td><a href="https://arxiv.org/abs/2403.11905">Tur[k]ingBench</a> presented @ NAACL 2025.</td>
          </tr>
          <tr>
            <td style="width:12%"><b>April 2025</b></td>
            <td><a href="https://arxiv.org/pdf/2504.03640?">Bonsai</a> now on arXiv.</td>
          </tr>
        </table>

      </section>
      <footer>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>

  </body>
</html>
